\documentclass{article}
\usepackage{graphicx, amsmath, amssymb} % Required for inserting images and math

\numberwithin{equation}{section} % Number equations within sections

\title{Kalman Filter}
\author{Sehwan Jhang}
\date{September 2024}

\begin{document}

\maketitle

\section{Averaging Filter}
Here is the original form of the mean:
\begin{equation}
    \bar{x}_k = \frac{x_1 + x_2 + \ldots + x_k}{k}
\end{equation}

Like equation (1.1), a calculation method where all the data is gathered and processed at once is referred to as a \textit{Batch expression}. But what happens if an additional data point is added? In this process, the previously calculated mean (\(\bar{x}_k\)) cannot be used.

\subsection{Recursive Expression of Mean}

\begin{equation}
    \bar{x}_{k-1} = \frac{x_1 + x_2 + \ldots + x_{k-1}}{k - 1}
\end{equation}

Multiply both sides of equation (1.1) by \textit{\textbf{k}}:
\begin{equation*}
    k \bar{x}_k = x_1 + x_2 + \ldots + x_k
\end{equation*}

Divide both sides of the equation by \textit{\textbf{k - 1}}:
\begin{equation*}
    \frac{k}{k-1} \bar{x}_k = \frac{x_1 + x_2 + \ldots + x_k}{k - 1}
\end{equation*}

Separate \( x_k \) and divide the right-hand side into two terms:
\begin{align*}
    \frac{k}{k-1} \bar{x}_k & = \frac{x_1 + x_2 + \ldots + x_k}{k - 1}                           \\
                            & = \frac{x_1 + x_2 + \ldots + x_{k - 1}}{k - 1} + \frac{x_k}{k - 1}
\end{align*}

However, the first term on the right-hand side is the same as the definition of \(\bar{x}_{k-1}\) from equation (1.2). Therefore, the above equation can be rewritten as follows:
\begin{equation}
    \frac{k}{k-1} \bar{x}_k = \bar{x}_{k-1} + \frac{x_k}{k - 1}
\end{equation}

Divide both sides of the equation by \(\frac{k}{k-1}\):
\begin{equation*}
    \bar{x}_k = \frac{k-1}{k} \bar{x}_{k-1} + \frac{x_k}{k}
\end{equation*}

Using equation (1.3) to calculating mean, we need to previous mean \(\bar{x}_{k-1}\), numbers of data \(k\) and new data \(x_k\)

To simplify equation (1.3).
Define \(\alpha \equiv \frac{k-1}{k}\)
\begin{align*}
    \alpha                & \equiv \frac{k - 1}{k} = 1 - \frac{1}{k} \\
    \therefore\frac{1}{k} & = 1 - \alpha
\end{align*}

\begin{align}
    \bar{x}_k & = \frac{k - 1}{k}\bar{x}_{k-1} + \frac{1}{k}x_k \nonumber \\
              & = \alpha \bar{x}_{k-1} + (1 - \alpha) x_k
\end{align}
The form of equation (1.4) is Averaging filter.

\section{Moving Average Filter}
The formula that numbers of data is \textit{\textbf{n}} for the moving average filter is:
\begin{equation}
    \bar{x}_k = \frac{x_{k-n+1} + x_{k-n+2} + \ldots + ...x_k}{n}
\end{equation}
Refer to equation (2.1), Definition of \(\bar{x}_{k-1}\) is:
\begin{equation}
    \bar{x}_{k-1} = \frac{x_{k-n}+x_{k-n+1}+\ldots+x_{k-1}}{n}
\end{equation}
Subtract equation (2.1) to (2.2);
\begin{align*}
    \bar{x}_k - \bar{x}_{k-1} & = \frac{x_{k-n+1}+x_{k-n+2}+\ldots+x_{k-1}+x_k}{n} \\
                              & - \frac{x_{k-n}+x_{k-n+1}+\ldots+x_{k-1}}{n}       \\
                              & = \frac{x_k - x_{k-n}}{n}
\end{align*}

Therefore, Simplify about \(\bar{x}_k\),
\begin{equation}
    \bar{x}_k = \bar{x}_{k-1} + \frac{x_k - x_{k-n}}{n}
\end{equation}

\section{Low-pass filter(first order)}
\subsection{The limits of moving average}

\begin{align*}
    \bar{x}_k & = \frac{x_{k-n+1} + x_{k-n+2} + \ldots + x_k}{n}                           \\
              & = \frac{1}{n} x_{k-n+1} + \frac{1}{n} x_{k-n+2} + \ldots + \frac{1}{n} x_k
\end{align*}
Moving average filter has limits. It doesn't accomplish to remove the noise and sensitivity of changes
Then, give higher weights to recent measurement and give lower weights to older measurement.

\begin{equation}
    \bar{x}_k = \alpha\bar{x}_{k-1} + (1 - \alpha)\bar{x}_k
\end{equation}
We consider the term \(\bar{x}_k\) to estimated value than mean.

\begin{equation}
    \bar{x}_{k-1} = \alpha\bar{x}_{k-2} + (1 - \alpha)\bar{x}_{k-1}
\end{equation}

Substitute equation (3.2) to equation (3.1)
\begin{align}
    \bar{x}_k & = \alpha\bar{x}_{k-1} + (1 - \alpha)x_k \nonumber                                 \\
              & = \alpha\{\alpha\bar{x}_{k-2} + (1 - \alpha)x_{k-1}\} + (1 - \alpha)x_k \nonumber \\
              & = \alpha^2\bar{x}_{k-2} + \alpha(1 - \alpha)\bar{x}_{k-1} + (1 - \alpha)x_k
\end{align}

Let's compare the coefficients \(\alpha(1-\alpha)\) and \(1 - \alpha\).Since \(\alpha\) is constant where \(0 < \alpha < 1 \),  the following relationship holds between the two coefficients:
\begin{align*}
    \alpha(1-\alpha) < 1 - \alpha
\end{align*}

By comparing this inequality with equation (3.3), we can see that the most recent measurement\( (x_k) \) is given more weight than the previous measurement \((x_{k-1})\) in the estimation. To be more precise, according to equation (3.1), \(x_{k-2}\) is defined as follows:

\begin{equation}
    \bar{x}_{k-2} = \alpha(1-\alpha)\bar{x}_{k-3} + (1-\alpha)x_{k-2}
\end{equation}

Substitute this equation to equation (3.3):
\begin{align}
    \bar{x}_k & = \alpha^{2}\bar{x}_{k-2} + \alpha(1-\alpha)x_{k-1}+ (1-\alpha)x_k \nonumber                                \\
              & = \alpha^{2}\{\alpha\bar{x}_{k-3} + (1-\alpha)x_{k-2}\} + \alpha(1-\alpha)x_{k-1} + (1-\alpha)x_k \nonumber \\
              & = \alpha^{3}x_{k-3} + \alpha^{2}(1-\alpha)x_{k-2} + \alpha(1-\alpha)x_{k-1} + (1-\alpha)x_k
\end{align}

Thus, the following relationship holds between the coefficients of the coefficients of estimated data:
\begin{align*}
    \alpha^{2}(1-\alpha) < \alpha(1-\alpha) < 1 - \alpha
\end{align*}

Thanks to this differentiation in weighting, the Low Pass Filter better meets the conflicting requirements of noise reduction and sensitivity to changes compared to the moving average filter.Like first order low-pass filter of equation (3.1) is called exponentially weighted moving average filter.As seen in equation (3.5), the moving average is calculated by assigning exponentially decreasing weights to older data, which is why this method is named accordingly.

\section{Kalman Filter}
\subsection{Kalman Filter Algorithm}
\begin{enumerate}
    \item Select the initial value
          \[\hat{x}_0, P_0\]
    \item  Estimated values and predict covariance
          \[\hat{x}^{-}_{k}=A\hat{x}_{k-1}\]
          \[P^{-}_{k}=AP_{k-1}A^{T} + Q\]
    \item Compute Kalman gain
          \[K_k = P^{-}_{k}H^{T}(HP^{-}_{k}H^{T} + R)^{-1}\]
          Measured value \(z_k\), Estimated value \(\hat{x}_k\)
    \item Compute Estimated value
          \[\hat{x}_k = \hat{x}^{-}_{k} + K_{k}(z_k-H\hat{x}^{-}_{k})\]
    \item Compute Error covariance
          \[P_k = P^{-}_{k} - K_kHP^{-}_{k}\]
    \item Back to Step 2 using \(\hat{x}_k\) and \(P_k\)
\end{enumerate}

The structure is simple: when a measurement value is input, it processes the data internally and then outputs the estimated value. The internal calculations are carried out in four steps.The superscript "-" denotes the predicted value.

\subsection{Calculate steps of kalman filter algorithm}

\begin{enumerate}
    \item Based on the system model (A, Q), the state and error covariance for the next time step are predicted.
          \[\hat{x}^{-}_{k}, P^{-}_{k}\]
    \item By correcting the difference between the measurement and the predicted value, a new estimate is calculated. This estimate is the final output of the Kalman filter.
          \[\hat{x}_{k}, P_k\]
    \item These two processes are repeated iteratively.
\end{enumerate}

\section{Estimate Process}
\subsection{Compute Estimated value}
\begin{equation}
    \hat{x}_{k} = \hat{x}^{-}_{k} + K_{k}(z_k - H\hat{x}^{-}_{k})
\end{equation}

\begin{align}
    \hat{x}_{k} & = \hat{x}^{-}_{k} + K_{k}(z_k - H\hat{x}^{-}_{k}) \nonumber      \\
                & = \hat{x}^{-}_{k} + K_{k}z_{k} - K_{k}H\hat{x}^{-}_{k} \nonumber \\
                & = (I - K_{k}H)\hat{x}^{-}_{k} + K_{k}z_{k}
\end{align}

Convert H to Identity matrix I
\begin{align}
    \hat{x}_{k} & = (I - K_{k}H)\hat{x}^{-}_{k} + K_{k}z_{k}  \nonumber \\
                & = (I - K_{k}I)\hat{x}^{-}_{k} + K_{k}z_{k} \nonumber  \\
                & = (I - K_{k})\hat{x}^{-}_{k} + K_{k}z_{k}
\end{align}

The equation (3.1) is:
\[\bar{x}_{k} = \alpha\bar{x}_{k-1} + (1 - \alpha)x_k\]

Substitute \(\alpha = 1 - K\)
\begin{align*}
    \bar{x}_{k} & = \alpha*\bar{x}_{k-1} + (1 - \alpha)x_{k}  \\
                & = (1 - K)\bar{x}_{k-1} + {1 - (1 - K)}x_{k} \\
                & = (1 - K)\bar{x}_{k-1} + Kx_k
\end{align*}
A first-order low-pass filter calculates the estimate by assigning weights to the previous estimate and the measured value,
and then adding them together. The Kalman filter is quite similar. It calculates the final estimate by multiplying the predicted value and the measured value by appropriate weights and then summing the two values.
The only difference is that it uses the predicted value instead of the previous estimate, but the method of applying weights is the same.

\[\hat{x}_{k} = \hat{x}^{-}_{k} + K_{k}(z_k - H\hat{x}^{-}_{k})\]
First, we need predict value(\(\hat{x}^{-}_{k}\)) and new measurement value(\(z_k\)).
The predicted value is obtained from the prediction process, which will be covered in the next section.
The system model is predetermined before designing the Kalman filter, so its values are also known in advance.
The last variable is \[K_k\]. It is called to Kalman gain.

\begin{equation}
    K_k = P_{k}^{-}H^{T}(HP_{k}^{-}H^{T} + R)^{-1}
\end{equation}

The weights in the estimation formula are adjusted each time.
\begin{equation}
    P_k = P_{k}^{-}H^{T} - K_{k}HP_{k}^{-}
\end{equation}

\begin{equation}
    x_{k} \sim N(\hat{x}_k, P_k)
\end{equation}

\begin{equation}
    P_{k} = E\{(x_{k} - \hat{x}^{K})(x_{k} - \hat{x}^{K})^{T}\}
\end{equation}
\end{document}
